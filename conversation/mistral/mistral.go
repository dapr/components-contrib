/*
Copyright 2024 The Dapr Authors
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

	http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/
package mistral

import (
	"context"
	"encoding/json"
	"fmt"
	"reflect"
	"strings"

	"github.com/tmc/langchaingo/llms"

	"github.com/dapr/components-contrib/conversation"
	"github.com/dapr/components-contrib/conversation/langchaingokit"
	"github.com/dapr/components-contrib/metadata"
	"github.com/dapr/kit/logger"
	kmeta "github.com/dapr/kit/metadata"

	mistral2 "github.com/gage-technologies/mistral-go"
	"github.com/tmc/langchaingo/llms/mistral"
)

type Mistral struct {
	langchaingokit.LLM

	logger logger.Logger
}

func usageGetter(resp *llms.ContentResponse) *conversation.UsageInfo {
	if resp == nil || len(resp.Choices) == 0 {
		return nil
	}

	choice := resp.Choices[0]
	usage, ok := (choice.GenerationInfo["usage"]).(mistral2.UsageInfo)
	if !ok {
		return nil
	}

	return &conversation.UsageInfo{
		PromptTokens:     int32(usage.PromptTokens),     //nolint:gosec // This is a valid conversion
		CompletionTokens: int32(usage.CompletionTokens), //nolint:gosec // This is a valid conversion
		TotalTokens:      int32(usage.TotalTokens),      //nolint:gosec // This is a valid conversion
	}
}

func NewMistral(logger logger.Logger) conversation.Conversation {
	m := &Mistral{
		logger: logger,
	}

	return m
}

const defaultModel = "open-mistral-7b"

func (m *Mistral) Init(ctx context.Context, meta conversation.Metadata) error {
	md := conversation.LangchainMetadata{}
	err := kmeta.DecodeMetadata(meta.Properties, &md)
	if err != nil {
		return err
	}

	model := defaultModel
	if md.Model != "" {
		model = md.Model
	}

	llm, err := mistral.New(
		mistral.WithModel(model),
		mistral.WithAPIKey(md.Key),
	)
	if err != nil {
		return err
	}

	m.LLM.Model = llm
	m.LLM.UsageGetterFunc = usageGetter

	if md.CacheTTL != "" {
		cachedModel, cacheErr := conversation.CacheModel(ctx, md.CacheTTL, m.LLM.Model)
		if cacheErr != nil {
			return cacheErr
		}

		m.LLM.Model = cachedModel
	}
	return nil
}

// Converse overrides the base LangChainGoKit implementation to provide Mistral-specific
// tool result processing. Mistral has strict conversation flow requirements and cannot
// process formal tool results for tool calls that weren't originally generated by Mistral.
// This method automatically converts tool results to text-based messages that Mistral can handle.
// TODO: This is a temporary workaround until langchaingo provides better native tool calling support for Mistral
func (m *Mistral) Converse(ctx context.Context, r *conversation.ConversationRequest) (*conversation.ConversationResponse, error) {
	if m.Model == nil {
		return nil, fmt.Errorf("LLM Model is nil - component not initialized")
	}

	// Convert tool results to text-based messages for Mistral compatibility
	convertedRequest := m.convertToolResultsToText(r)

	// Build messages using custom Mistral-compatible processing
	messages := m.getMistralCompatibleMessages(convertedRequest)

	// Extract tools from inputs
	var tools []conversation.Tool
	for _, input := range convertedRequest.Inputs {
		if len(input.Parts) > 0 {
			if toolDefs := conversation.ExtractToolDefinitionsFromParts(input.Parts); len(toolDefs) > 0 {
				tools = toolDefs
			}
		}
	}

	// Build options
	opts := langchaingokit.GetOptionsFromRequest(convertedRequest)

	// Add tools if provided
	if len(tools) > 0 {
		langchainTools := m.convertDaprToolsToLangchainTools(tools)
		opts = append(opts, llms.WithTools(langchainTools))
	}

	// Call LangChain model
	resp, err := m.GenerateContent(ctx, messages, opts...)
	if err != nil {
		return nil, err
	}

	// Process response
	outputs := make([]conversation.ConversationResult, 0, len(resp.Choices))

	// Determine the primary finish reason from the first choice
	var primaryFinishReason string
	if len(resp.Choices) > 0 {
		primaryFinishReason = resp.Choices[0].StopReason
	}

	for i := range resp.Choices {
		result := conversation.ConversationResult{
			Parameters: convertedRequest.Parameters,
		}

		// Create response parts
		var parts []conversation.ContentPart

		// Add text content if available
		if resp.Choices[i].Content != "" {
			parts = append(parts, conversation.TextContentPart{Text: resp.Choices[i].Content})
		}

		// Add tool calls if present
		if len(resp.Choices[i].ToolCalls) > 0 {
			for _, tc := range resp.Choices[i].ToolCalls {
				// Generate provider-compatible ID if not provided by the provider
				toolCallID := tc.ID
				if toolCallID == "" {
					toolCallID = conversation.GenerateProviderCompatibleToolCallID()
				}

				parts = append(parts, conversation.ToolCallContentPart{
					ID:       toolCallID,
					CallType: tc.Type,
					Function: conversation.ToolCallFunction{
						Name:      tc.FunctionCall.Name,
						Arguments: tc.FunctionCall.Arguments,
					},
				})
			}
		}

		// Set content parts and legacy result field
		result.Parts = parts
		result.Result = conversation.ExtractTextFromParts(parts)

		// Set finish reason
		if primaryFinishReason != "" {
			result.FinishReason = primaryFinishReason
		} else if resp.Choices[i].StopReason != "" {
			result.FinishReason = resp.Choices[i].StopReason
		} else {
			result.FinishReason = conversation.DefaultFinishReason(parts)
		}

		outputs = append(outputs, result)
	}

	// Get usage information
	usageGetter := m.UsageGetterFunc
	if usageGetter == nil {
		usageGetter = conversation.ExtractUsageFromResponse
	}

	return &conversation.ConversationResponse{
		Outputs: outputs,
		Usage:   usageGetter(resp),
	}, nil
}

// getMistralCompatibleMessages builds messages with Mistral-specific processing
// This ensures tool results are converted to text before reaching LangChain
func (m *Mistral) getMistralCompatibleMessages(r *conversation.ConversationRequest) []llms.MessageContent {
	messages := make([]llms.MessageContent, 0, len(r.Inputs))

	for _, input := range r.Inputs {
		role := langchaingokit.ConvertLangchainRole(input.Role)

		// Process with native parts support if available
		if len(input.Parts) > 0 {
			var textParts []string
			var toolCalls []llms.ToolCall

			for _, part := range input.Parts {
				switch p := part.(type) {
				case conversation.TextContentPart:
					textParts = append(textParts, p.Text)
				case conversation.ToolCallContentPart:
					toolCalls = append(toolCalls, llms.ToolCall{
						ID:   p.ID,
						Type: p.CallType,
						FunctionCall: &llms.FunctionCall{
							Name:      p.Function.Name,
							Arguments: p.Function.Arguments,
						},
					})
					// Note: ToolResultContentPart should already be converted to text by convertToolResultsToText
				}
			}

			// Create appropriate message type based on content
			if len(toolCalls) > 0 {
				// Assistant message with tool calls
				message := llms.MessageContent{
					Role: llms.ChatMessageTypeAI,
				}

				// Add text content if present
				if len(textParts) > 0 {
					message.Parts = []llms.ContentPart{llms.TextPart(strings.Join(textParts, " "))}
				}

				// Include tool calls for conversation context
				for _, toolCall := range toolCalls {
					message.Parts = append(message.Parts, toolCall)
				}

				messages = append(messages, message)
			} else if len(textParts) > 0 {
				// Regular text message
				messages = append(messages, llms.MessageContent{
					Role:  role,
					Parts: []llms.ContentPart{llms.TextPart(strings.Join(textParts, " "))},
				})
			}
		} else if input.Message != "" {
			// Fallback to legacy message processing
			messages = append(messages, llms.MessageContent{
				Role: role,
				Parts: []llms.ContentPart{
					llms.TextPart(input.Message),
				},
			})
		}
	}

	return messages
}

// convertDaprToolsToLangchainTools converts Dapr tool definitions to langchaingo format
func (m *Mistral) convertDaprToolsToLangchainTools(tools []conversation.Tool) []llms.Tool {
	if len(tools) == 0 {
		return nil
	}

	langchainTools := make([]llms.Tool, len(tools))
	for i, tool := range tools {
		langchainTools[i] = llms.Tool{
			Type: tool.ToolType,
			Function: &llms.FunctionDefinition{
				Name:        tool.Function.Name,
				Description: tool.Function.Description,
				Parameters:  m.convertParametersToMap(tool.Function.Parameters),
			},
		}
	}
	return langchainTools
}

// convertParametersToMap converts tool parameters from JSON string to map[string]any if needed
func (m *Mistral) convertParametersToMap(params any) any {
	// If params is already a map, return it as-is
	if paramMap, ok := params.(map[string]any); ok {
		return paramMap
	}

	// If params is a string, try to unmarshal it as JSON
	if paramStr, ok := params.(string); ok {
		var paramMap map[string]any
		if err := json.Unmarshal([]byte(paramStr), &paramMap); err != nil {
			// If unmarshaling fails, return original string
			return params
		}
		return paramMap
	}

	// For other types, return as-is
	return params
}

// convertToolResultsToText converts formal tool results to text-based messages
// that Mistral can process. This maintains the conversation flow while working
// around Mistral's limitation with simulated tool calls.
// TODO: This is a temporary workaround. See conversation/langchaingokit/LANGCHAINGO_WORKAROUNDS.md
func (m *Mistral) convertToolResultsToText(r *conversation.ConversationRequest) *conversation.ConversationRequest {
	convertedInputs := make([]conversation.ConversationInput, 0, len(r.Inputs))

	for _, input := range r.Inputs {
		// Check if this input contains tool results
		if len(input.Parts) > 0 {
			var hasToolResults bool
			var toolResults []conversation.ToolResultContentPart

			// Identify tool result parts
			for _, part := range input.Parts {
				if resultPart, ok := part.(conversation.ToolResultContentPart); ok {
					hasToolResults = true
					toolResults = append(toolResults, resultPart)
				}
			}

			if hasToolResults {
				// Convert tool results to text-based user message
				var textParts []string

				for _, result := range toolResults {
					var resultText string
					if result.IsError {
						resultText = fmt.Sprintf("The %s function returned an error: %s", result.Name, result.Content)
					} else {
						resultText = fmt.Sprintf("The %s function returned: %s", result.Name, result.Content)
					}
					textParts = append(textParts, resultText)
				}

				// Add instruction for the AI to use this information
				textParts = append(textParts, "Please use this information to respond to the user.")

				// Create a text-based user message
				convertedInput := conversation.ConversationInput{
					Role: conversation.RoleUser,
					Parts: []conversation.ContentPart{
						conversation.TextContentPart{
							Text: strings.Join(textParts, " "),
						},
					},
				}
				convertedInputs = append(convertedInputs, convertedInput)

				m.logger.Debugf("Converted tool result to text for Mistral: %s", convertedInput.Parts[0].(conversation.TextContentPart).Text)
			} else {
				// No tool results, keep the input as-is
				convertedInputs = append(convertedInputs, input)
			}
		} else {
			// No parts or legacy message, keep as-is
			convertedInputs = append(convertedInputs, input)
		}
	}

	// Return new request with converted inputs
	return &conversation.ConversationRequest{
		Inputs:              convertedInputs,
		Parameters:          r.Parameters,
		Temperature:         r.Temperature,
		ConversationContext: r.ConversationContext,
	}
}

func (m *Mistral) GetComponentMetadata() (metadataInfo metadata.MetadataMap) {
	metadataStruct := conversation.LangchainMetadata{}
	metadata.GetMetadataInfoFromStructType(reflect.TypeOf(metadataStruct), &metadataInfo, metadata.ConversationType)
	return
}

func (m *Mistral) Close() error {
	return nil
}
