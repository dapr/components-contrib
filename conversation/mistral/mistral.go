/*
Copyright 2024 The Dapr Authors
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

	http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/
package mistral

import (
	"context"
	"encoding/json"
	"errors"
	"fmt"
	"reflect"
	"strings"

	mistral2 "github.com/gage-technologies/mistral-go"
	"github.com/tmc/langchaingo/llms"
	"github.com/tmc/langchaingo/llms/mistral"

	"github.com/dapr/components-contrib/conversation"
	"github.com/dapr/components-contrib/conversation/langchaingokit"
	"github.com/dapr/components-contrib/metadata"
	"github.com/dapr/kit/logger"
	kmeta "github.com/dapr/kit/metadata"
)

type Mistral struct {
	langchaingokit.LLM

	logger logger.Logger
}

func usageGetter(resp *llms.ContentResponse) *conversation.UsageInfo {
	if resp == nil || len(resp.Choices) == 0 {
		return nil
	}

	choice := resp.Choices[0]
	usage, ok := (choice.GenerationInfo["usage"]).(mistral2.UsageInfo)
	if !ok {
		return nil
	}

	return &conversation.UsageInfo{
		PromptTokens:     uint64(usage.PromptTokens),     //nolint:gosec // This is a valid conversion
		CompletionTokens: uint64(usage.CompletionTokens), //nolint:gosec // This is a valid conversion
		TotalTokens:      uint64(usage.TotalTokens),      //nolint:gosec // This is a valid conversion
	}
}

func NewMistral(logger logger.Logger) conversation.Conversation {
	return &Mistral{
		logger: logger,
	}
}

const defaultModel = "mistral-medium-2505"

func (m *Mistral) Init(ctx context.Context, meta conversation.Metadata) error {
	md := conversation.LangchainMetadata{}
	err := kmeta.DecodeMetadata(meta.Properties, &md)
	if err != nil {
		return err
	}

	model := defaultModel
	if md.Model != "" {
		model = md.Model
	}

	key := md.Key
	if key == "" {
		key = conversation.GetEnvKey("MISTRAL_API_KEY")
		if key == "" {
			return errors.New("mistral key is required")
		}
	}

	llm, err := mistral.New(
		mistral.WithModel(model),
		mistral.WithAPIKey(key),
	)
	if err != nil {
		return err
	}

	m.LLM.Model = llm
	m.LLM.ProviderModelName = "mistral/" + model
	m.LLM.UsageGetterFunc = usageGetter

	if md.CacheTTL != "" {
		cachedModel, cacheErr := conversation.CacheModel(ctx, md.CacheTTL, m.LLM.Model)
		if cacheErr != nil {
			return cacheErr
		}

		m.LLM.Model = cachedModel
	}
	return nil
}

// Converse overrides the base LangChainGoKit implementation to provide Mistral-specific
// tool result processing. Mistral has strict conversation flow requirements and cannot
// process formal tool results for tool calls that weren't originally generated by Mistral.
// This method automatically converts tool results to text-based messages that Mistral can handle.
// TODO: This is a temporary workaround until langchaingo provides better native tool calling support for Mistral
func (m *Mistral) Converse(ctx context.Context, r *conversation.ConversationRequest) (*conversation.ConversationResponse, error) {
	if m.Model == nil {
		return nil, errors.New("LLM Model is nil - component not initialized")
	}

	// Convert tool results to text-based messages for Mistral compatibility
	convertedRequest := m.convertToolResultsToText(r)

	// Build messages using custom Mistral-compatible processing
	messages := m.getMistralCompatibleMessages(convertedRequest)

	// Get tools from the request (new API structure)
	tools := convertedRequest.Tools

	// Note: Tools are now only supported in ConversationRequest.Tools field

	// Build options
	opts := langchaingokit.GetOptionsFromRequest(convertedRequest)

	// Add tools if provided
	if len(tools) > 0 {
		langchainTools := m.convertDaprToolsToLangchainTools(tools)
		opts = append(opts, llms.WithTools(langchainTools))
	}

	// Call LangChain model
	resp, err := m.GenerateContent(ctx, messages, opts...)
	if err != nil {
		return nil, err
	}

	// Process response
	outputs := make([]conversation.ConversationOutput, 0, len(resp.Choices))

	// Determine the primary finish reason from the first choice
	var primaryFinishReason string
	if len(resp.Choices) > 0 {
		primaryFinishReason = resp.Choices[0].StopReason
	}

	for i := range resp.Choices {
		result := conversation.ConversationOutput{
			Parameters: convertedRequest.Parameters,
		}

		// Create response parts
		var parts []conversation.ContentPart

		// Add text content if available
		if resp.Choices[i].Content != "" {
			parts = append(parts, conversation.TextContentPart{Text: resp.Choices[i].Content})
		}

		// Add tool calls if present
		if len(resp.Choices[i].ToolCalls) > 0 {
			for _, tc := range resp.Choices[i].ToolCalls {
				// Generate provider-compatible ID if not provided by the provider
				toolCallID := tc.ID
				if toolCallID == "" {
					toolCallID = conversation.GenerateProviderCompatibleToolCallID()
				}

				parts = append(parts, conversation.ToolCallContentPart{
					ID:       toolCallID,
					CallType: tc.Type,
					Function: conversation.ToolCallFunction{
						Name:      tc.FunctionCall.Name,
						Arguments: tc.FunctionCall.Arguments,
					},
				})
			}
		}

		// Set content parts and legacy result field
		result.Parts = parts
		result.Result = conversation.ExtractTextFromParts(parts) //nolint:staticcheck // Backward compatibility

		// Set finish reason
		if primaryFinishReason != "" {
			result.FinishReason = conversation.NormalizeFinishReason(primaryFinishReason)
		} else if resp.Choices[i].StopReason != "" {
			result.FinishReason = conversation.NormalizeFinishReason(resp.Choices[i].StopReason)
		} else {
			result.FinishReason = conversation.DefaultFinishReason(parts)
		}

		outputs = append(outputs, result)
	}

	// Get usage information
	usgGetter := m.UsageGetterFunc
	if usgGetter == nil {
		usgGetter = conversation.ExtractUsageFromResponse
	}

	return &conversation.ConversationResponse{
		Outputs: outputs,
		Usage:   usgGetter(resp),
	}, nil
}

// getMistralCompatibleMessages builds messages with Mistral-specific processing
// This ensures tool results are processed correctly by langchaingo
func (m *Mistral) getMistralCompatibleMessages(r *conversation.ConversationRequest) []llms.MessageContent {
	messages := make([]llms.MessageContent, 0, len(r.Inputs))

	i := 0
	for i < len(r.Inputs) {
		input := r.Inputs[i]
		role := langchaingokit.ConvertLangchainRole(input.Role)

		// Process with native parts support if available
		if len(input.Parts) > 0 {
			var textParts []string
			var toolCalls []llms.ToolCall
			var toolResults []conversation.ToolResultContentPart

			for _, part := range input.Parts {
				switch p := part.(type) {
				case conversation.TextContentPart:
					textParts = append(textParts, p.Text)
				case conversation.ToolCallContentPart:
					toolCalls = append(toolCalls, llms.ToolCall{
						ID:   p.ID,
						Type: p.CallType,
						FunctionCall: &llms.FunctionCall{
							Name:      p.Function.Name,
							Arguments: p.Function.Arguments,
						},
					})
				case conversation.ToolResultContentPart:
					toolResults = append(toolResults, p)
				}
			}

			// Handle tool results - group consecutive tool result inputs
			if len(toolResults) > 0 {
				// Look ahead to collect all consecutive tool result inputs
				allToolResults := toolResults
				j := i + 1

				// Collect consecutive tool result inputs
				for j < len(r.Inputs) {
					nextInput := r.Inputs[j]
					if len(nextInput.Parts) > 0 {
						var nextHasToolResults bool
						var nextToolResults []conversation.ToolResultContentPart

						for _, part := range nextInput.Parts {
							if resultPart, ok := part.(conversation.ToolResultContentPart); ok {
								nextHasToolResults = true
								nextToolResults = append(nextToolResults, resultPart)
							}
						}

						if nextHasToolResults && len(nextInput.Parts) == len(nextToolResults) {
							// This input contains only tool results, collect them
							allToolResults = append(allToolResults, nextToolResults...)
							j++
						} else {
							// Mixed content or no tool results, stop collecting
							break
						}
					} else {
						// No parts, stop collecting
						break
					}
				}

				// Create a single tool message with all collected tool results
				var toolParts []llms.ContentPart
				for _, result := range allToolResults {
					toolParts = append(toolParts, llms.ToolCallResponse{
						ToolCallID: result.ToolCallID,
						Name:       result.Name,
						Content:    result.Content,
					})
				}

				messages = append(messages, llms.MessageContent{
					Role:  llms.ChatMessageTypeTool,
					Parts: toolParts,
				})

				// Skip all the inputs we processed
				i = j
			} else if len(toolCalls) > 0 {
				// Assistant message with tool calls
				message := llms.MessageContent{
					Role: llms.ChatMessageTypeAI,
				}

				// Add text content if present
				if len(textParts) > 0 {
					message.Parts = []llms.ContentPart{llms.TextPart(strings.Join(textParts, " "))}
				}

				// Include tool calls for conversation context
				for _, toolCall := range toolCalls {
					message.Parts = append(message.Parts, toolCall)
				}

				messages = append(messages, message)
				i++
			} else if len(textParts) > 0 {
				// Regular text message
				messages = append(messages, llms.MessageContent{
					Role:  role,
					Parts: []llms.ContentPart{llms.TextPart(strings.Join(textParts, " "))},
				})
				i++
			} else {
				// Empty message, skip
				i++
			}
		} else if input.Message != "" { //nolint:staticcheck // Backward compatibility check
			// Fallback to legacy message processing
			messages = append(messages, llms.MessageContent{
				Role: role,
				Parts: []llms.ContentPart{
					llms.TextPart(input.Message), //nolint:staticcheck // Backward compatibility
				},
			})
			i++
		} else {
			// Empty input, skip
			i++
		}
	}

	return messages
}

// convertDaprToolsToLangchainTools converts Dapr tool definitions to langchaingo format
func (m *Mistral) convertDaprToolsToLangchainTools(tools []conversation.Tool) []llms.Tool {
	if len(tools) == 0 {
		return nil
	}

	langchainTools := make([]llms.Tool, len(tools))
	for i, tool := range tools {
		toolType := tool.ToolType
		if toolType == "" {
			toolType = "function"
		}
		langchainTools[i] = llms.Tool{
			Type: toolType,
			Function: &llms.FunctionDefinition{
				Name:        tool.Function.Name,
				Description: tool.Function.Description,
				Parameters:  m.convertParametersToMap(tool.Function.Parameters),
			},
		}
	}
	return langchainTools
}

// convertParametersToMap converts tool parameters from JSON string to map[string]any if needed
func (m *Mistral) convertParametersToMap(params any) any {
	// If params is already a map, return it as-is
	if paramMap, ok := params.(map[string]any); ok {
		return paramMap
	}

	// If params is a string, try to unmarshal it as JSON
	if paramStr, ok := params.(string); ok {
		var paramMap map[string]any
		if err := json.Unmarshal([]byte(paramStr), &paramMap); err != nil {
			// If unmarshaling fails, return original string
			return params
		}
		return paramMap
	}

	// For other types, return as-is
	return params
}

// convertToolResultsToText converts formal tool results to text-based messages
// that Mistral can process. This maintains the conversation flow while working
// around Mistral's limitation with simulated tool calls.
// Groups consecutive tool results into a single text-based user message.
// Also handles Mistral's API limitation with tool call history by creating fresh context.
// TODO: This is a temporary workaround. See conversation/langchaingokit/LANGCHAINGO_WORKAROUNDS.md
func (m *Mistral) convertToolResultsToText(r *conversation.ConversationRequest) *conversation.ConversationRequest {
	convertedInputs := make([]conversation.ConversationInput, 0, len(r.Inputs))

	// Check if conversation history contains tool calls (Mistral API limitation)
	hasToolCallHistory := false
	for _, input := range r.Inputs {
		if input.Role == conversation.RoleAssistant && len(input.Parts) > 0 {
			for _, part := range input.Parts {
				if _, ok := part.(conversation.ToolCallContentPart); ok {
					hasToolCallHistory = true
					break
				}
			}
		}
		if hasToolCallHistory {
			break
		}
	}

	// If we have tool call history, create a fresh conversation context
	if hasToolCallHistory {
		m.logger.Debugf("Mistral: Detected tool call history, creating fresh conversation context")

		// Extract the original user request and convert tool results to context
		var originalUserRequest string
		var toolResultTexts []string
		var lastUserInput conversation.ConversationInput

		for _, input := range r.Inputs {
			switch input.Role {
			case conversation.RoleUser:
				// Keep track of the last user input as it might be the current request
				lastUserInput = input
				if len(input.Parts) > 0 {
					for _, part := range input.Parts {
						if textPart, ok := part.(conversation.TextContentPart); ok {
							originalUserRequest = textPart.Text
						}
					}
				}
			case conversation.RoleTool:
				// Extract tool result information
				for _, part := range input.Parts {
					if resultPart, ok := part.(conversation.ToolResultContentPart); ok {
						var resultText string
						if resultPart.IsError {
							resultText = fmt.Sprintf("Error from %s: %s", resultPart.Name, resultPart.Content)
						} else {
							resultText = fmt.Sprintf("%s result: %s", resultPart.Name, resultPart.Content)
						}
						toolResultTexts = append(toolResultTexts, resultText)
					}
				}
			}
		}

		// Create a fresh conversation with tool results as context
		if len(toolResultTexts) > 0 {
			contextText := fmt.Sprintf("Based on previous tool execution results: %s. ", strings.Join(toolResultTexts, ". "))
			if originalUserRequest != "" {
				contextText += "User's original request was: " + originalUserRequest
			} else if len(lastUserInput.Parts) > 0 {
				// Use the last user input if available
				for _, part := range lastUserInput.Parts {
					if textPart, ok := part.(conversation.TextContentPart); ok {
						contextText += "User's current request: " + textPart.Text
						break
					}
				}
			}

			// Create fresh conversation input
			convertedInputs = append(convertedInputs, conversation.ConversationInput{
				Role: conversation.RoleUser,
				Parts: []conversation.ContentPart{
					conversation.TextContentPart{Text: contextText},
				},
			})

			m.logger.Debugf("Mistral: Created fresh context with tool results: %s", contextText)
		} else {
			// No tool results found, just use the last user input
			convertedInputs = append(convertedInputs, lastUserInput)
		}

		// Return new request with fresh context
		return &conversation.ConversationRequest{
			Inputs:              convertedInputs,
			Parameters:          r.Parameters,
			Temperature:         r.Temperature,
			ConversationContext: r.ConversationContext,
			Tools:               r.Tools,
		}
	}

	// If no tool call history, proceed with normal tool result conversion
	i := 0
	for i < len(r.Inputs) {
		input := r.Inputs[i]

		// Check if this input contains tool results
		if len(input.Parts) > 0 {
			var hasToolResults bool
			var toolResults []conversation.ToolResultContentPart

			// Identify tool result parts in current input
			for _, part := range input.Parts {
				if resultPart, ok := part.(conversation.ToolResultContentPart); ok {
					hasToolResults = true
					toolResults = append(toolResults, resultPart)
				}
			}

			if hasToolResults {
				// Look ahead to collect all consecutive tool result inputs
				allToolResults := toolResults
				j := i + 1

				// Collect consecutive tool result inputs
				for j < len(r.Inputs) {
					nextInput := r.Inputs[j]
					if len(nextInput.Parts) > 0 {
						var nextHasToolResults bool
						var nextToolResults []conversation.ToolResultContentPart

						for _, part := range nextInput.Parts {
							if resultPart, ok := part.(conversation.ToolResultContentPart); ok {
								nextHasToolResults = true
								nextToolResults = append(nextToolResults, resultPart)
							}
						}

						if nextHasToolResults && len(nextInput.Parts) == len(nextToolResults) {
							// This input contains only tool results, collect them
							allToolResults = append(allToolResults, nextToolResults...)
							j++
						} else {
							// Mixed content or no tool results, stop collecting
							break
						}
					} else {
						// No parts, stop collecting
						break
					}
				}

				// Convert all collected tool results to a single text-based user message
				var textParts []string

				for _, result := range allToolResults {
					var resultText string
					if result.IsError {
						resultText = fmt.Sprintf("The %s function returned an error: %s", result.Name, result.Content)
					} else {
						resultText = fmt.Sprintf("The %s function returned: %s", result.Name, result.Content)
					}
					textParts = append(textParts, resultText)
				}

				// Add instruction for the AI to use this information
				textParts = append(textParts, "Please use this information to respond to the user.")

				// Create a single text-based user message for all tool results
				convertedInput := conversation.ConversationInput{
					Role: conversation.RoleUser,
					Parts: []conversation.ContentPart{
						conversation.TextContentPart{
							Text: strings.Join(textParts, " "),
						},
					},
				}
				convertedInputs = append(convertedInputs, convertedInput)

				m.logger.Debugf("Converted %d tool result(s) to text for Mistral: %s", len(allToolResults), convertedInput.Parts[0].(conversation.TextContentPart).Text)

				// Skip all the inputs we processed
				i = j
			} else {
				// No tool results, keep the input as-is
				convertedInputs = append(convertedInputs, input)
				i++
			}
		} else {
			// No parts or legacy message, keep as-is
			convertedInputs = append(convertedInputs, input)
			i++
		}
	}

	// Return new request with converted inputs
	return &conversation.ConversationRequest{
		Inputs:              convertedInputs,
		Parameters:          r.Parameters,
		Temperature:         r.Temperature,
		ConversationContext: r.ConversationContext,
		Tools:               r.Tools,
	}
}

func (m *Mistral) GetComponentMetadata() (metadataInfo metadata.MetadataMap) {
	metadataStruct := conversation.LangchainMetadata{}
	metadata.GetMetadataInfoFromStructType(reflect.TypeOf(metadataStruct), &metadataInfo, metadata.ConversationType)
	return
}

func (m *Mistral) Close() error {
	return nil
}
